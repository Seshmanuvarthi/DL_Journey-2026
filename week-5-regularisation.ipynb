{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31286,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-23T07:38:19.389797Z","iopub.execute_input":"2026-02-23T07:38:19.390092Z","iopub.status.idle":"2026-02-23T07:38:19.414912Z","shell.execute_reply.started":"2026-02-23T07:38:19.390070Z","shell.execute_reply":"2026-02-23T07:38:19.414307Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/datasets/organizations/zalando-research/fashionmnist/t10k-labels-idx1-ubyte\n/kaggle/input/datasets/organizations/zalando-research/fashionmnist/t10k-images-idx3-ubyte\n/kaggle/input/datasets/organizations/zalando-research/fashionmnist/fashion-mnist_test.csv\n/kaggle/input/datasets/organizations/zalando-research/fashionmnist/fashion-mnist_train.csv\n/kaggle/input/datasets/organizations/zalando-research/fashionmnist/train-labels-idx1-ubyte\n/kaggle/input/datasets/organizations/zalando-research/fashionmnist/train-images-idx3-ubyte\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"zalando-research/fashionmnist\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T07:38:19.416077Z","iopub.execute_input":"2026-02-23T07:38:19.416312Z","iopub.status.idle":"2026-02-23T07:38:19.985840Z","shell.execute_reply.started":"2026-02-23T07:38:19.416292Z","shell.execute_reply":"2026-02-23T07:38:19.985219Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/datasets/organizations/zalando-research/fashionmnist\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import layers, models, regularizers\n\n# 1. LOAD DATA\ntrain_df = pd.read_csv('/kaggle/input/datasets/organizations/zalando-research/fashionmnist/fashion-mnist_train.csv')\ntest_df = pd.read_csv('/kaggle/input/datasets/organizations/zalando-research/fashionmnist/fashion-mnist_test.csv')\n\n# 2. PREPROCESS DATA\n# Extract labels (y) and pixel values (X)\ny_train_raw = train_df.iloc[:, 0].values\nx_train_raw = train_df.iloc[:, 1:].values\n\ny_test_raw = test_df.iloc[:, 0].values\nx_test_raw = test_df.iloc[:, 1:].values\n\n# Normalize pixel values to [0, 1] as per Unit I guidelines\nx_train = x_train_raw.astype(\"float32\") / 255.0\nx_test = x_test_raw.astype(\"float32\") / 255.0\n\n# One-hot encode labels (Unit II Requirement)\ny_train = to_categorical(y_train_raw, 10)\ny_test = to_categorical(y_test_raw, 10)\n\n# 3. BUILD MODEL WITH L2 REGULARIZATION\ndef build_l2_model():\n    model = models.Sequential([\n        layers.Dense(128, activation='relu', input_shape=(784,),\n                     kernel_regularizer=regularizers.l2(0.01)),\n        layers.Dense(64, activation='relu',\n                     kernel_regularizer=regularizers.l2(0.01)),\n        layers.Dense(10, activation='softmax')\n    ])\n    return model\n\nmodel_l2 = build_l2_model()\n\n# 4. COMPILE AND TRAIN\nmodel_l2.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model_l2.fit(\n    x_train, y_train,\n    epochs=10,\n    validation_split=0.2, # Monitoring for overfitting\n    batch_size=32\n)\n\n# 5. EVALUATE\ntest_loss, test_acc = model_l2.evaluate(x_test, y_test)\nprint(f\"\\n--- Fashion-MNIST Results ---\")\nprint(f\"Test accuracy: {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T07:38:19.986555Z","iopub.execute_input":"2026-02-23T07:38:19.986757Z","iopub.status.idle":"2026-02-23T07:39:06.097068Z","shell.execute_reply.started":"2026-02-23T07:38:19.986738Z","shell.execute_reply":"2026-02-23T07:39:06.096435Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7345 - loss: 1.6552 - val_accuracy: 0.7940 - val_loss: 0.7770\nEpoch 2/10\n\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8024 - loss: 0.7442 - val_accuracy: 0.8200 - val_loss: 0.6956\nEpoch 3/10\n\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8172 - loss: 0.6804 - val_accuracy: 0.8227 - val_loss: 0.6636\nEpoch 4/10\n\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8211 - loss: 0.6384 - val_accuracy: 0.8050 - val_loss: 0.6824\nEpoch 5/10\n\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8295 - loss: 0.6154 - val_accuracy: 0.8311 - val_loss: 0.6078\nEpoch 6/10\n\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8293 - loss: 0.6050 - val_accuracy: 0.8360 - val_loss: 0.5878\nEpoch 7/10\n\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8305 - loss: 0.5886 - val_accuracy: 0.8332 - val_loss: 0.5958\nEpoch 8/10\n\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8365 - loss: 0.5725 - val_accuracy: 0.8313 - val_loss: 0.5866\nEpoch 9/10\n\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8367 - loss: 0.5678 - val_accuracy: 0.8443 - val_loss: 0.5605\nEpoch 10/10\n\u001b[1m1500/1500\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8362 - loss: 0.5610 - val_accuracy: 0.8389 - val_loss: 0.5625\n\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8370 - loss: 0.5551\n\n--- Fashion-MNIST Results ---\nTest accuracy: 0.8383\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import fashion_mnist # Changed from mnist\nfrom tensorflow.keras import layers, models, regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# 1. Load Fashion MNIST\n(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n\n# 2. Preprocess\n# Normalize to [0, 1]\nx_train = x_train.astype(\"float32\") / 255.0\nx_test = x_test.astype(\"float32\") / 255.0\n\n# Reshape for ImageDataGenerator (Height, Width, Channels)\nx_train = x_train.reshape(-1, 28, 28, 1)\nx_test = x_test.reshape(-1, 28, 28, 1)\n\n# One-hot encoding for the 10 clothing classes\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\n\n# 3. Data Augmentation (Modified for Fashion Data)\ndatagen = ImageDataGenerator(\n    rotation_range=10,\n    zoom_range=0.1,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,  # Set to True for clothes! \n    vertical_flip=False    # Keep False (we don't wear shirts upside down)\n)\n\n\n\ndatagen.fit(x_train)\n\n# 4. Build MLP Model with L2 Regularization\ndef build_l2_model():\n    model = models.Sequential([\n        layers.Flatten(input_shape=(28, 28, 1)),\n        # Adding more neurons (128) because Fashion-MNIST is more complex than digits\n        layers.Dense(128, activation='relu',\n                     kernel_regularizer=regularizers.l2(0.001)), \n        layers.Dropout(0.2), # Adding Dropout as per Week 5 Syllabus\n        layers.Dense(64, activation='relu',\n                     kernel_regularizer=regularizers.l2(0.001)),\n        layers.Dense(10, activation='softmax')\n    ])\n    return model\n\nmodel_l2 = build_l2_model()\n\n# 5. Compile\nmodel_l2.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# 6. Train using the generator\nhistory = model_l2.fit(\n    datagen.flow(x_train, y_train, batch_size=32),\n    epochs=15, # Increased epochs as augmentation makes learning slower but more robust\n    validation_data=(x_test, y_test),\n    steps_per_epoch=len(x_train) // 32\n)\n\n# 7. Evaluate\ntest_loss, test_acc = model_l2.evaluate(x_test, y_test)\nprint(f\"\\nFashion-MNIST Test accuracy: {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T07:39:06.098610Z","iopub.execute_input":"2026-02-23T07:39:06.098864Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  25/1875\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - accuracy: 0.2370 - loss: 2.4171","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Check if GPU is available\ndevice_name = tf.test.gpu_device_name()\nif device_name != '/device:GPU:0':\n    print('GPU device not found. Check settings!')\nelse:\n    print('Found GPU at: {}'.format(device_name))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Early Stopping","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# 1. LOAD DAta\ntrain_df = pd.read_csv('/kaggle/input/datasets/organizations/zalando-research/fashionmnist/fashion-mnist_train.csv')\ntest_df = pd.read_csv('/kaggle/input/datasets/organizations/zalando-research/fashionmnist/fashion-mnist_test.csv')\n\n\n\n# 2. PREPROCESS\ny_train = to_categorical(train_df.iloc[:, 0].values, 10)\nx_train = train_df.iloc[:, 1:].values.astype(\"float32\") / 255.0\n\ny_test = to_categorical(test_df.iloc[:, 0].values, 10)\nx_test = test_df.iloc[:, 1:].values.astype(\"float32\") / 255.0\n\n# 3. BUILD MODEL\ndef build_model():\n    model = models.Sequential([\n        # Fashion-MNIST is complex; using 128 neurons for better representation\n        layers.Dense(128, activation='relu', input_shape=(784,)),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(10, activation='softmax')\n    ])\n    return model\n\nmodel = build_model()\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# 4. EARLY STOPPING (Unit II Regularization)\n# This prevents overfitting by stopping training when validation loss stops decreasing\nearlystop = EarlyStopping(\n    monitor='val_loss',       # Monitoring loss is usually better for early stopping\n    patience=5,               # Give it 5 epochs to improve before quitting\n    restore_best_weights=True \n)\n\n\n\n# 5. TRAIN\nhistory = model.fit(\n    x_train, y_train,\n    epochs=50,                # Set high; EarlyStopping will handle the cut-off\n    batch_size=256,\n    validation_split=0.2,\n    callbacks=[earlystop],\n    verbose=1\n)\n\n# 6. EVALUATE & VISUALIZE\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(f\"\\nâœ… Fashion-MNIST Test accuracy: {test_acc:.4f}\")\n\n# Plotting Loss to visualize Early Stopping for your lab report\nplt.figure(figsize=(10, 4))\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Effect of Early Stopping on Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Drop out","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import layers, models\n\n# 1. LOAD FASHION-MNIST (Using local Kaggle paths)\ntrain_df = pd.read_csv('/kaggle/input/datasets/organizations/zalando-research/fashionmnist/fashion-mnist_train.csv')\ntest_df = pd.read_csv('/kaggle/input/datasets/organizations/zalando-research/fashionmnist/fashion-mnist_test.csv')\n\n\n# 2. PREPROCESS\ny_train = to_categorical(train_df.iloc[:, 0].values, 10)\nx_train = train_df.iloc[:, 1:].values.astype(\"float32\") / 255.0\n\ny_test = to_categorical(test_df.iloc[:, 0].values, 10)\nx_test = test_df.iloc[:, 1:].values.astype(\"float32\") / 255.0\n\n# 3. BUILD MODEL WITH DROPOUT\n# Note: Since you have a GPU, we can increase the neurons to 256 \n# to give the model more 'learning capacity' before Dropout trims it down.\ndef build_dropout_model():\n    model = models.Sequential([\n        layers.Dense(256, activation='relu', input_shape=(784,)),\n        layers.Dropout(0.3),  # ğŸ”¹ Randomly sets 30% of inputs to 0 during training\n        layers.Dense(128, activation='relu'),\n        layers.Dropout(0.3),  # ğŸ”¹ Helps prevent neurons from co-depending too much\n        layers.Dense(10, activation='softmax')\n    ])\n    return model\n\n# [Image of dropout regularization in neural networks showing active and inactive neurons]\n\n# 4. COMPILE (Using Adam as per your syllabus)\nmodel = build_dropout_model()\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# 5. TRAIN \n# With the GPU Tesla T4, we can handle a larger batch size (512) for faster training\nhistory = model.fit(\n    x_train, y_train,\n    epochs=25, \n    batch_size=512,\n    validation_split=0.2,\n    verbose=1\n)\n\n# 6. EVALUATE\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(f\"\\n Fashion-MNIST with Dropout Test accuracy: {test_acc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Adding Noise","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import layers, models\n\n# 1. LOAD DAAta\ntrain_df = pd.read_csv('/kaggle/input/datasets/organizations/zalando-research/fashionmnist/fashion-mnist_train.csv')\ntest_df = pd.read_csv('/kaggle/input/datasets/organizations/zalando-research/fashionmnist/fashion-mnist_test.csv')\n\n# 2. PREPROCESS CLEAN DATA\ny_train = to_categorical(train_df.iloc[:, 0].values, 10)\nx_train = train_df.iloc[:, 1:].values.astype(\"float32\") / 255.0\n\ny_test = to_categorical(test_df.iloc[:, 0].values, 10)\nx_test = test_df.iloc[:, 1:].values.astype(\"float32\") / 255.0\n\n# ===============================\n# ğŸ”¹ 3. ADD NOISE TO INPUTS (Unit II Regularization)\n# ===============================\n# We add Gaussian noise to the images\nnoise_factor = 0.25 \nx_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n\n# Clip back to [0, 1] range\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\n\n# \n\n# ===============================\n# ğŸ”¹ 4. ADD NOISE TO OUTPUTS (Label Noise)\n# We intentionally 'lie' to the model for 10% of labels to test robustness\n# ===============================\nnoise_ratio = 0.1\nnum_samples = int(noise_ratio * y_train.shape[0])\nrandom_indices = np.random.choice(y_train.shape[0], num_samples, replace=False)\n\nfor idx in random_indices:\n    # Generate a random label from 0 to 9\n    random_label = np.random.randint(0, 10)\n    y_train[idx] = to_categorical(random_label, 10)\n\n# ===============================\n# 5. BUILD MODEL\n# ===============================\ndef build_model():\n    model = models.Sequential([\n        # Since input is noisy, a slightly larger layer helps capture signal\n        layers.Dense(128, activation='relu', input_shape=(784,)),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(10, activation='softmax')\n    ])\n    return model\n\nmodel = build_model()\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# 6. TRAIN (On Noisy Data, validating on Clean Data)\n# Using GPU power with batch_size 512\nhistory = model.fit(\n    x_train_noisy, \n    y_train, \n    epochs=25, \n    batch_size=512, \n    validation_data=(x_test, y_test), # Check if model generalizes to clean data\n    verbose=1\n)\n\n# 7. EVALUATE\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(f\"\\n Robustness Test Results (Fashion-MNIST)\")\nprint(f\"Test accuracy on Clean Data: {test_acc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Parameter sharing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Input, Dense, Concatenate\nfrom tensorflow.keras.models import Model\n\n# 1. LOAD DATA\ntrain_df = pd.read_csv('/kaggle/input/datasets/organizations/zalando-research/fashionmnist/fashion-mnist_train.csv')\ntest_df = pd.read_csv('/kaggle/input/datasets/organizations/zalando-research/fashionmnist/fashion-mnist_test.csv')\n\n# 2. PREPROCESS\ny_train = to_categorical(train_df.iloc[:, 0].values, 10)\nx_train = train_df.iloc[:, 1:].values.astype(\"float32\") / 255.0\n\ny_test = to_categorical(test_df.iloc[:, 0].values, 10)\nx_test = test_df.iloc[:, 1:].values.astype(\"float32\") / 255.0\n\n# 3. BUILD SHARED ARCHITECTURE (Functional API)\n# Parameter Sharing: The 'shared_dense' layer has ONE set of weights (W, b)\nshared_dense = Dense(128, activation='relu', name=\"shared_feature_extractor\")\n\n# Define two input branches\ninput_a = Input(shape=(784,), name=\"Input_Branch_A\")\ninput_b = Input(shape=(784,), name=\"Input_Branch_B\")\n\n# Both inputs use the SAME layer instance (Weights are tied/shared)\nfeat_a = shared_dense(input_a)\nfeat_b = shared_dense(input_b)\n\n# Merge the shared representations\nmerged = Concatenate()([feat_a, feat_b])\n\n# Final Classification Head\nhidden = Dense(64, activation='relu')(merged)\noutput = Dense(10, activation='softmax')(merged)\n\n# Construct Model\nmodel = Model(inputs=[input_a, input_b], outputs=output)\n\n\n# 4. COMPILE & TRAIN\nmodel.compile(optimizer='adam', \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])\n\n# Train (Using GPU: Tesla T4)\n# In this demo, we feed the same image to both branches. \n# In a real project, you might feed an image and its augmented version!\nhistory = model.fit([x_train, x_train], y_train,\n                    epochs=15,\n                    batch_size=512,\n                    validation_split=0.2)\n\n# 5. EVALUATE\ntest_loss, test_acc = model.evaluate([x_test, x_test], y_test)\nprint(f\"\\nFashion-MNIST Parameter Sharing Accuracy: {test_acc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}